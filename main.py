import base64
import json
import logging
import os
from datetime import datetime

import pandas as pd
from flask import Flask, request, make_response
from google.cloud import storage, firestore
from io import StringIO

# ------------------------
# App & Logging
# ------------------------
app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# ------------------------
# Clients
# ------------------------
storage_client = storage.Client()
firestore_client = firestore.Client()

# ------------------------
# Constants
# ------------------------
RAW_PREFIX = "raw-data/"
REPORTS_PREFIX = "reports/"
PROCESSED_COLLECTION = "processed_files"

# ------------------------
# Helper Functions
# ------------------------
def already_processed(filename: str) -> bool:
    safe_doc_id = filename.replace("/", "_")
    doc_ref = firestore_client.collection(PROCESSED_COLLECTION).document(safe_doc_id)
    return doc_ref.get().exists



def mark_processed(filename: str):
    safe_doc_id = filename.replace("/", "_")
    firestore_client.collection(PROCESSED_COLLECTION).document(safe_doc_id).set({
        "processed_at": firestore.SERVER_TIMESTAMP
    })



def compute_metrics(df: pd.DataFrame) -> dict:
    return {
        "row_count": int(len(df)),
        "null_counts": df.isnull().sum().to_dict(),
        "columns": list(df.columns),
    }


def write_report(bucket_name: str, source_file: str, metrics: dict):
    bucket = storage_client.bucket(bucket_name)
    report_name = source_file.replace(RAW_PREFIX, REPORTS_PREFIX).replace(".csv", ".json")

    blob = bucket.blob(report_name)
    blob.upload_from_string(
        json.dumps(metrics, indent=2),
        content_type="application/json"
    )

    logging.info("Metrics written to %s", report_name)


# ------------------------
# Cloud Run Entry Point
# ------------------------
@app.route("/", methods=["POST"])
def gcs_event_handler():
    """
    Handles Pub/Sub PUSH messages generated by GCS notifications.
    """

    envelope = request.get_json(silent=True)

    # Validate Pub/Sub envelope
    if not envelope or "message" not in envelope:
        logging.error("Invalid Pub/Sub message format")
        return make_response("Bad Request", 400)

    message = envelope["message"]

    # Decode message data (optional for GCS notifications)
    if "data" in message:
        try:
            decoded_data = base64.b64decode(message["data"]).decode("utf-8")
            logging.info("Decoded Pub/Sub data: %s", decoded_data)
        except Exception:
            logging.warning("Unable to decode message data")

    attributes = message.get("attributes", {})
    bucket_name = attributes.get("bucketId")
    object_name = attributes.get("objectId")

    if not bucket_name or not object_name:
        logging.error("Missing bucket or object information")
        return make_response("Bad Request", 400)

    logging.info("Bucket: %s", bucket_name)
    logging.info("Object: %s", object_name)

    # ------------------------
    # CSV Validation
    # ------------------------
    if not object_name.startswith(RAW_PREFIX):
        logging.info("Ignoring file outside raw-data/")
        return make_response("OK", 200)

    if not object_name.lower().endswith(".csv"):
        logging.info("Ignoring non-CSV file")
        return make_response("OK", 200)

    # ------------------------
    # Idempotency Check
    # ------------------------
    if already_processed(object_name):
        logging.info("File already processed, skipping: %s", object_name)
        return make_response("OK", 200)

    try:
        # ------------------------
        # Read CSV from GCS
        # ------------------------
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(object_name)
        csv_content = blob.download_as_text()

        df = pd.read_csv(StringIO(csv_content))

        # ------------------------
        # Compute Metrics
        # ------------------------
        metrics = compute_metrics(df)
        metrics.update({
            "source_file": object_name,
            "processed_at": datetime.utcnow().isoformat()
        })

        # ------------------------
        # Write JSON Report
        # ------------------------
        write_report(bucket_name, object_name, metrics)

        # ------------------------
        # Mark as Processed
        # ------------------------
        mark_processed(object_name)

        logging.info("Successfully processed %s", object_name)
        return make_response("OK", 200)

    except Exception as e:
        logging.exception("Processing failed: %s", e)

        # IMPORTANT:
        # Return 500 so Pub/Sub retries (as per assignment requirement)
        return make_response("Internal Error", 500)
